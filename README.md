# ART-CoreEngine
By Benjamin Carter, Dylan Johnson, Hunter Jenkins, and Jacob McAuley Penney
Parser and code understanding engine

> Please run `onClone.sh` when repo is cloned for the first time!!!!

## ART Project Overview:

- This project consists of two parts. The training portion and the prediction engine
- The prediction engine is designed to be imported into other projects

## Prediction Engine:
- See `example_external.py` for how to use. Should just be imports and paths to the
required files generated by the model training
- See below steps 1 and 2 for setting up the environment

## Setting up the environment (for training):
1. Run `bash on_clone.sh` -- this creates some of the folder structures
2. Run `poetry install` -- this sets up the virtual environment
3. Set up a configuration file for training -- 

``` json
{
    "repo": "Your Repository Here",
    "auth_path": "File to your github auth key",
    "output_path": "Output to extractor json (not currently in use)",
    "gpt_jsonl_path" : "output/gpt_jsonl_path.json",
    "api_domain_label_listing" : "File with domain listing",
    "api_subdomain_label_listing" : "File with domain and subdomain listing",
    "classification_method" : "Either 'rf' or 'gpt', determines the model to train",
    "classification_model_save" : "Model output file", 
    "range": [
        1,
        
    ],
    "state": "closed",
    "comments": [
        "userid",
        "userlogin",
        "body"
    ],
    "commits": [
        "author_name",
        "committer",
        "date",
        "sha",
        "message",
        "files"
    ],
    "issues": [
        "userid",
        "userlogin",
        "title",
        "body",
        "num_comments",
        "created_at",
        "closed_at"
    ]
}
```
4. Set an environemnt variable in a `.env` file with the `OPENAI_API_KEY` set to an OpenAI key
5. Run `poetry run python3 main.py path/to/config.json` where the json is the one set 
up from step three. This will download, analyze, and train the model. It stores the results in 
a cache, preventing repeated calls. It is recommended to delete the generated `main.db` file 
in the output directory when switching between repositories for training.


> If you want to see the AI calls in real time, run
> `tail -n 100 -f output/ai_log.csv`



## output format:
- `ai_log.csv` This is the log of all AI calls done.
- `main.db` This is the main SQLite file that manages all the run artifacts. Deleting this file will prompt complete environment regeneration.
- `ai_cache_results.db` This will be a backup persistent database. Not recommended to delete, as deleting will result in replayed calls to OpenAI
- `downloadedFiles/*` Everything in this is temporary. This stores all the processed files from the program.

> :warning: **Warning**<br>
SAVE AND BACKUP both `ai_log.csv` and `ai_result_backup.db` in the `output` directory as this keeps track of AI artifacts. Deleting this file can result in having to redo OpenAI calls, costing money!

> :information_source: **Info**<br>
If you want to restart the analysis, delete **ONLY** the `main.db` file in the `output` directory.

